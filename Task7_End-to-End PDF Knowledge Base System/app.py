import os
import uuid
from datetime import datetime
from flask import Flask, request, jsonify, render_template, send_from_directory
from werkzeug.utils import secure_filename
import asyncio # For running async LLM calls

# Import modules
from pdf_processor import extract_text_from_pdf, clean_text, chunk_text
from nlp_processor import extract_entities, summarize_text, extract_keywords
from es_manager import ElasticsearchManager
from vector_db_manager import VectorDBManager # NEW
from llm_rag import LLMRag # NEW

# --- Configuration ---
UPLOAD_FOLDER = 'uploads' # Directory to temporarily store uploaded PDFs
ALLOWED_EXTENSIONS = {'pdf'}
ES_HOST = "http://localhost:9200" # Your Elasticsearch host
ES_INDEX_NAME = "pdf_knowledge_base"
CHROMA_PERSIST_DIR = "chroma_db" # Directory for ChromaDB persistence

app = Flask(__name__)
app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER

# Ensure upload folder exists
os.makedirs(UPLOAD_FOLDER, exist_ok=True)
os.makedirs(CHROMA_PERSIST_DIR, exist_ok=True) # Ensure ChromaDB directory exists

# Initialize Elasticsearch Manager
es_manager = ElasticsearchManager(es_host=ES_HOST, index_name=ES_INDEX_NAME)
es_manager.create_index() # Ensure ES index exists on startup

# Initialize VectorDB Manager
vector_db_manager = VectorDBManager(persist_directory=CHROMA_PERSIST_DIR)

# Initialize LLM RAG
llm_rag = LLMRag(vector_db_manager=vector_db_manager)


def allowed_file(filename):
    """Checks if the uploaded file has an allowed extension."""
    return '.' in filename and \
           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

# --- Routes ---

@app.route('/')
def index():
    """Renders the main HTML page for PDF upload, search, and RAG."""
    return render_template('index.html')

@app.route('/upload_pdf', methods=['POST'])
async def upload_pdf():
    """
    Handles PDF file uploads, processes them, and indexes the information
    into both Elasticsearch (full document) and ChromaDB (chunks).
    """
    if 'pdf_file' not in request.files:
        return jsonify({"error": "No file part"}), 400

    file = request.files['pdf_file']

    if file.filename == '':
        return jsonify({"error": "No selected file"}), 400

    if file and allowed_file(file.filename):
        filename = secure_filename(file.filename)
        filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        file.save(filepath)

        doc_id = str(uuid.uuid4()) # Unique ID for the full document

        try:
            # 1. PDF Parsing and Text Cleaning
            full_text = extract_text_from_pdf(filepath)
            if not full_text:
                os.remove(filepath)
                return jsonify({"error": "Could not extract text from PDF. OCR might be needed for scanned documents."}), 500

            cleaned_text = clean_text(full_text)

            # 2. Information Extraction (NLP) for full document metadata
            entities = extract_entities(cleaned_text)
            summary = summarize_text(cleaned_text)
            keywords = extract_keywords(cleaned_text)

            title = filename.replace('.pdf', '').replace('_', ' ').title()
            author = "Unknown" # Placeholder
            publication_date = datetime.now().isoformat() # Current timestamp

            # 3. Prepare Full Document for Elasticsearch Indexing
            document_data = {
                "filename": filename,
                "title": title,
                "author": author,
                "publication_date": publication_date,
                "full_text": cleaned_text,
                "summary": summary,
                "keywords": keywords,
                "persons": entities["persons"],
                "organizations": entities["organizations"],
                "locations": entities["locations"],
                "tables": [], # Placeholder for structured data
                "figures": [], # Placeholder for structured data
                "timestamp": datetime.now().isoformat()
            }

            # 4. Storage to Elasticsearch (full document)
            es_manager.index_document(doc_id, document_data)

            # --- NEW: Chunking and Vector DB Indexing ---
            chunks = chunk_text(cleaned_text)
            chunk_ids = []
            chunk_metadatas = []
            
            for i, chunk in enumerate(chunks):
                chunk_id = f"{doc_id}_chunk_{i}"
                chunk_ids.append(chunk_id)
                chunk_metadatas.append({
                    "document_id": doc_id,
                    "filename": filename,
                    "chunk_index": i,
                    "title": title,
                    "summary": summarize_text(chunk, max_length=50) # Small summary for chunk metadata
                })
            
            # Add chunks to ChromaDB. Embeddings are generated by ChromaDB's embedding function.
            vector_db_manager.add_chunks(chunk_ids, chunks, chunk_metadatas)

            # Clean up temporary file
            os.remove(filepath)

            return jsonify({"message": "PDF processed and indexed successfully (ES & ChromaDB)", "doc_id": doc_id, "chunks_indexed": len(chunks)}), 200

        except Exception as e:
            # Clean up temporary file in case of error
            if os.path.exists(filepath):
                os.remove(filepath)
            print(f"Error during PDF processing: {e}")
            return jsonify({"error": f"Failed to process PDF: {e}"}), 500
    else:
        return jsonify({"error": "Invalid file type"}), 400

@app.route('/search', methods=['GET'])
def search():
    """
    Handles traditional full-text search queries for documents in Elasticsearch.
    """
    query = request.args.get('q', '')
    if not query:
        return jsonify({"error": "Query parameter 'q' is required"}), 400

    try:
        results = es_manager.search_documents(query)
        return jsonify(results), 200
    except Exception as e:
        print(f"Error during Elasticsearch search: {e}")
        return jsonify({"error": f"Search failed: {e}"}), 500

@app.route('/ask_rag', methods=['GET'])
async def ask_rag():
    """
    Handles RAG (Retrieval Augmented Generation) queries.
    """
    query = request.args.get('q', '')
    if not query:
        return jsonify({"error": "Query parameter 'q' is required"}), 400

    try:
        # Asynchronously call the RAG function
        answer = await llm_rag.answer_question_with_rag(query)
        return jsonify({"answer": answer}), 200
    except Exception as e:
        print(f"Error during RAG query: {e}")
        return jsonify({"error": f"Failed to answer question: {e}"}), 500

@app.route('/document/<doc_id>', methods=['GET'])
def get_document(doc_id):
    """
    Retrieves a specific document by its ID from Elasticsearch.
    """
    try:
        document = es_manager.get_document_by_id(doc_id)
        if document:
            return jsonify(document), 200
        else:
            return jsonify({"error": "Document not found"}), 404
    except Exception as e:
        print(f"Error retrieving document from ES: {e}")
        return jsonify({"error": f"Failed to retrieve document: {e}"}), 500

# --- Main execution ---
if __name__ == '__main__':
    # Use asyncio.run for running async functions in a sync context if needed
    # For Flask, if you want to run async routes directly, you need an ASGI server like Hypercorn or Gunicorn with Uvicorn worker.
    # For simple development, Flask's built-in server can handle async routes in Python 3.7+ (though not truly async I/O).
    app.run(debug=True, host='0.0.0.0', port=5000)